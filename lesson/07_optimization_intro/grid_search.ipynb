{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8012eb-1c7a-4823-8db7-33608795538b",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "\n",
    "Numerical optimization is a key part of modern computing. Even Machine Learning and Generative AI models \"learn\" by optimizing.\n",
    "\n",
    "The most primitive way to optimize is a grid search. It is easy to understand and easy to code up. Occasionally, it may even be useful!\n",
    "\n",
    "## Basic of the basics: one-dimensional exhaustive grid search\n",
    "\n",
    "One way to characterize optimization is that you are trying to find parameters for a function so that the output of that function meets some condition as closely as possible.\n",
    "\n",
    "A very simple case is trying to find zeros for a polynomial.\n",
    "\n",
    "Zero-finding is also at the heart of most of the more general optimization cases, as well. If you are trying to maximize or minimize some \"objective function,\" and that function is differentiable, then usually that means you're looking for a point where the first derivatives of that function are zero, too.\n",
    "\n",
    "But sometimes we just want to know zeros for their own sake! Let's start with a couple of easy polynomials, with obvious analytical solutions for $f(x) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "14d3de4d-e76f-49b8-bd05-b4842bf5da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "easy_cubic = lambda x: x**3 + 3\n",
    "\n",
    "N_grid = 100\n",
    "x_a = -10\n",
    "x_b = 10\n",
    "\n",
    "x_grid = np.linspace(x_a,x_b,N_grid)\n",
    "\n",
    "zero_ind = np.argmin(np.abs(easy_cubic(x_grid) - 0))\n",
    "\n",
    "optimal_x = x_grid[zero_ind]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab95216-8729-4fd5-a862-4c4e08c398ed",
   "metadata": {},
   "source": [
    "### Quick exercise for exhaustive grid search:\n",
    "  1. Organize the above starter code into something more re-useable. A function, for example!\n",
    "  2. Compute the distance of the optimized function value from the target--in this case, zero.\n",
    "  3. Use the analytical formula to compute the true root for $x^3 + 3$, and the distance of the optimal solution from this true solution.\n",
    "  4. Vary the number of grid points. How many grid points do you need to get what you consider to be an acceptably close answer?\n",
    "  5. Try a different function which also has an analytical solution. Is accuracy similar, or different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec11f8-0a0c-4527-be0a-ec8e77993608",
   "metadata": {},
   "source": [
    "## Iterative grid search\n",
    "\n",
    "We can improve accuracy and performance by upgrading to an iterative grid search. For this approach, we will use a relatively sparse grid, then make a new grid around the best point, iteratively until we get \"close enough\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3772bc90-835b-4a67-bc11-7bf1039c6799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10 10\n",
      "-1.5151515151515156 -1.3131313131313131\n",
      "-1.4416896235078056 -1.4437302316090197\n",
      "-1.4422461529899548 -1.4422667651929975\n",
      "-1.442249484255093 -1.4422496924591641\n",
      "-1.4422495704810214 -1.44224956837795\n",
      "-1.4422495703110763 -1.442249570289833\n",
      "-1.4422495703074285 -1.4422495703072138\n",
      "-1.442249570307409 -1.4422495703074067\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "N_grid = 100\n",
    "x_a = -10\n",
    "x_b = 10\n",
    "\n",
    "tol = 1e-16\n",
    "maxits = 10000\n",
    "\n",
    "diff = float('inf')\n",
    "\n",
    "its = 0\n",
    "\n",
    "while (diff > tol) and (its < maxits):\n",
    "    print(x_a,x_b)\n",
    "    x_grid = np.linspace(x_a,x_b,N_grid)\n",
    "    best_ind = np.argmin(np.abs(easy_cubic(x_grid) - 0))\n",
    "    x_a = x_grid[best_ind]\n",
    "    x_grid = np.delete(x_grid,best_ind)\n",
    "    second_best_ind = np.argmin(np.abs(easy_cubic(x_grid) - 0))\n",
    "    x_b = x_grid[second_best_ind]\n",
    "    diff = np.abs(x_a-x_b)\n",
    "    its += 1\n",
    "\n",
    "print(its)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc51e9-70e3-4caf-8f66-14028912e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "870f7505-2f22-4dd4-ac71-c25f42bb77db",
   "metadata": {},
   "source": [
    "### Quick exercise for iterative grid search:\n",
    "  1. Organize the above starter code into something more re-useable. A function, for example!\n",
    "  2. As you did before, compute the distance of the optimized function value from the target and of the optimal solution from the true solution.\n",
    "  4. Vary the number of grid points. How many grid points is too few? How many grid points is \"enough\"?\n",
    "  5. Compare the iterative algorithm against the exhaustive, in terms of both accuracy and computational performance.\n",
    "  6. Pick a tricky function which does not have an analytical solution for the zero. Find the zero. Plot your result, to \"prove\" that your solution is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd0277-b08f-4227-8514-a37240614997",
   "metadata": {},
   "source": [
    "## Multi-dimensional grid search\n",
    "\n",
    "Many optimization problems are multi-dimensional. AI model optimization is famously extremely high-dimensional!\n",
    "\n",
    "Let's start with two dimensions, and an easy bivariate polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "427b2839-4a0e-421f-9810-1a1f392862f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "easy_bivar_quad = lambda x,y: x**2 + y**2\n",
    "\n",
    "N_grid = 100\n",
    "x_a = -10\n",
    "x_b = 10\n",
    "\n",
    "x_grid = np.reshape(np.linspace(x_a,x_b,N_grid),(N_grid,1)) @ np.ones((1,N_grid))\n",
    "y_grid = np.ones((N_grid,1)) @ np.reshape(np.linspace(x_a,x_b,N_grid),(1,N_grid))\n",
    "\n",
    "zero_x,zero_y = np.unravel_index(\n",
    "    np.argmin(np.abs(easy_bivar_quad(x_grid,y_grid) - 0),\n",
    "              axis=None),\n",
    "    x_grid.shape)\n",
    "\n",
    "optimal_x = x_grid[zero_x,zero_y]\n",
    "optimal_y = y_grid[zero_x,zero_y]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4acf10-b390-4eaf-bf9d-2c84cba229fd",
   "metadata": {},
   "source": [
    "### Quick exercise for two-dimensional grid search:\n",
    "  1. Organize the above starter code into something more re-useable. A function, for example!\n",
    "  2. As you did before, compute the distance of the optimized function value from the target and of the optimal solution from the true solution.\n",
    "  3. Vary the number of grid points. How many grid points is \"enough\"? How does execution time vary as grid density increases?\n",
    "  4. Pick a different bivariate function with an interesting maximum or minimum. Modify the algorithm to search for a minimim or maximum, rather than a zero. How well does it perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c1e12-1b6a-4a37-9788-52e32ab533a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
